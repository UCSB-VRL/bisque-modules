{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f\"/home/bowen68/projects/bisque/ByteTrack\")\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "import supervision\n",
    "import os\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from utils.general import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervision.draw.color import ColorPalette\n",
    "from supervision.geometry.dataclasses import Point\n",
    "from supervision.video.dataclasses import VideoInfo\n",
    "from supervision.video.source import get_video_frames_generator\n",
    "from supervision.video.sink import VideoSink\n",
    "from supervision.notebook.utils import show_frame_in_notebook\n",
    "# from supervision.tools.detections import BoxAnnotator\n",
    "from supervision.tools.detections import Detections, BoxAnnotator\n",
    "from supervision.tools.line_counter import LineCounter, LineCounterAnnotator\n",
    "# from tracking_utils import Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
    "from onemetric.cv.utils.iou import box_iou_batch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BYTETrackerArgs:\n",
    "    track_thresh: float = 0.25\n",
    "    track_buffer: int = 30\n",
    "    match_thresh: float = 0.8\n",
    "    aspect_ratio_thresh: float = 3.0\n",
    "    min_box_area: float = 1.0\n",
    "    mot20: bool = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# converts Detections into format that can be consumed by match_detections_with_tracks function\n",
    "def detections2boxes(detections: Detections) -> np.ndarray:\n",
    "    return np.hstack((\n",
    "        detections.xyxy,\n",
    "        detections.confidence[:, np.newaxis]\n",
    "    ))\n",
    "\n",
    "\n",
    "# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
    "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
    "    return np.array([\n",
    "        track.tlbr\n",
    "        for track\n",
    "        in tracks\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "# matches our bounding boxes with predictions\n",
    "def match_detections_with_tracks(\n",
    "    detections: Detections, \n",
    "    tracks: List[STrack]\n",
    ") -> Detections:\n",
    "    if not np.any(detections.xyxy) or len(tracks) == 0:\n",
    "        return np.empty((0,))\n",
    "\n",
    "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
    "    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n",
    "    track2detection = np.argmax(iou, axis=1)\n",
    "    \n",
    "    tracker_ids = [None] * len(detections)\n",
    "    \n",
    "    for tracker_index, detection_index in enumerate(track2detection):\n",
    "        if iou[tracker_index, detection_index] != 0:\n",
    "            tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
    "\n",
    "    return tracker_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.torch_utils import select_device, time_sync\n",
    "from models.common import DetectMultiBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2023-2-20 torch 1.13.1 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11020MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46156743 parameters, 0 gradients, 107.8 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 1088, 1920)\n"
     ]
    }
   ],
   "source": [
    "device = ''\n",
    "weights = 'runs/train/bowen-run-27-new/weights/best.pt'\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "data = 'data/mare.yaml'\n",
    "data = os.path.join(root_dir, data)\n",
    "device = select_device(device)\n",
    "\n",
    "\n",
    "model = DetectMultiBackend(weights, device=device, data=data)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = (1088, 1920)\n",
    "model.warmup(imgsz=(1, 3, *imgsz))\n",
    "print((1, 3, *imgsz))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoInfo(width=1920, height=1080, fps=30, total_frames=882)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.augmentations import letterbox #padded resize \n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "CLASS_ID = [0,1,2,3,4,5,6,7,8,9]\n",
    "HOME = os.getcwd()\n",
    "# SOURCE_VIDEO_PATH = f\"{HOME}/examples/test_video.mp4\"\n",
    "SOURCE_VIDEO_PATH = f\"{HOME}/examples/example30s.mp4\"\n",
    "TARGET_VIDEO_PATH = f\"{HOME}/examples/output/example30s.mp4\"\n",
    "VideoInfo.from_video_path(SOURCE_VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres = 0.25\n",
    "iou_thres = 0.45\n",
    "max_det = 1000\n",
    "\n",
    "# create frame generator\n",
    "generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "# create instance of BoxAnnotator\n",
    "box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n",
    "# acquire first video frame\n",
    "iterator = iter(generator)\n",
    "frame0 = next(iterator)\n",
    "# padded resize from (1080 X 1920) to (1088 X 1920)\n",
    "frame = frame0 / 255\n",
    "frame = letterbox(frame, imgsz, stride=32, auto=True)[0]\n",
    "# (1088 x 1920 x 3) -> (3 x 1088 x 1920)\n",
    "frame = frame.transpose((2, 0, 1))\n",
    "frame = np.ascontiguousarray(frame)\n",
    "frame = torch.from_numpy(frame).to(device).float()\n",
    "frame = frame[None] # expand for batch dim\n",
    "# model prediction on single frame and conversion to supervision Detections\n",
    "pred0 = model(frame, augment=None, visualize=False)\n",
    "\n",
    "# pred = non_max_suppression(pred0, conf_thres, iou_thres, max_det=max_det)\n",
    "pred = non_max_suppression(pred0, conf_thres, iou_thres, None, False, max_det=max_det)\n",
    "# x_min, y_min, x_max, y_max, confidence, class_id in pred[0].cpu().numpy():\n",
    "det = pred[0].cpu().numpy()\n",
    "xyxy = det[:,:4]\n",
    "confidence = det[:,4]\n",
    "class_id = det[:,5].astype(int)\n",
    "\n",
    "detections = Detections(\n",
    "    xyxy=xyxy,\n",
    "    confidence=confidence,\n",
    "    class_id=class_id\n",
    ")\n",
    "# format custom labels\n",
    "labels = [\n",
    "    f\"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "    for _, confidence, class_id, tracker_id\n",
    "    in detections\n",
    "]\n",
    "# # annotate and display frame\n",
    "frame = box_annotator.annotate(frame=frame0, detections=detections, labels=labels)\n",
    "\n",
    "%matplotlib inline\n",
    "show_frame_in_notebook(frame0, (16, 16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b062a0110cbd40488cd45ef734c046f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/882 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m pred0 \u001b[39m=\u001b[39m model(frame, augment\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, visualize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[39m# pred = non_max_suppression(pred0, conf_thres, iou_thres, max_det=max_det)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m pred \u001b[39m=\u001b[39m non_max_suppression(pred0, conf_thres, iou_thres, \u001b[39mNone\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m, max_det\u001b[39m=\u001b[39;49mmax_det)\n\u001b[1;32m     40\u001b[0m det \u001b[39m=\u001b[39m pred[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     41\u001b[0m xyxy \u001b[39m=\u001b[39m det[:,:\u001b[39m4\u001b[39m]\n",
      "File \u001b[0;32m~/projects/bisque/SpeciesDetector/src/utils/general.py:773\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det)\u001b[0m\n\u001b[1;32m    770\u001b[0m x \u001b[39m=\u001b[39m x[xc[xi]]  \u001b[39m# confidence\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39m# Cat apriori labels if autolabelling\u001b[39;00m\n\u001b[0;32m--> 773\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(labels[xi]):\n\u001b[1;32m    774\u001b[0m     lb \u001b[39m=\u001b[39m labels[xi]\n\u001b[1;32m    775\u001b[0m     v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(lb), nc \u001b[39m+\u001b[39m \u001b[39m5\u001b[39m), device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SOURCE_VIDEO_PATH = f\"{HOME}/examples/example30s.mp4\"\n",
    "TARGET_VIDEO_PATH = f\"{HOME}/examples/output/example30s.mp4\"\n",
    "\n",
    "conf_thres = 0.25\n",
    "iou_thres = 0.45\n",
    "max_det = 1000\n",
    "LINE_START = Point(50, 800)\n",
    "LINE_END = Point(1920-50, 800)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# create BYTETracker instance\n",
    "byte_tracker = BYTETracker(BYTETrackerArgs())\n",
    "# create VideoInfo instance\n",
    "video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "# create frame generator\n",
    "generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "# create LineCounter instance\n",
    "line_counter = LineCounter(start=LINE_START, end=LINE_END)\n",
    "# create instance of BoxAnnotator and LineCounterAnnotator\n",
    "box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n",
    "line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "\n",
    "# open target video file\n",
    "with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    # loop over video frames\n",
    "    for frame0 in tqdm(generator, total=video_info.total_frames):\n",
    "        # model prediction on single frame and conversion to supervision Detections\n",
    "        frame = frame0 / 255\n",
    "        frame = letterbox(frame, imgsz, stride=32, auto=True)[0]\n",
    "        # (1088 x 1920 x 3) -> (3 x 1088 x 1920)\n",
    "        frame = frame.transpose((2, 0, 1))\n",
    "        frame = np.ascontiguousarray(frame)\n",
    "        frame = torch.from_numpy(frame).to(device).float()\n",
    "        frame = frame[None] # expand for batch dim\n",
    "        # model prediction on single frame and conversion to supervision Detections\n",
    "        pred0 = model(frame, augment=None, visualize=False)\n",
    "\n",
    "        # pred = non_max_suppression(pred0, conf_thres, iou_thres, max_det=max_det)\n",
    "        pred = non_max_suppression(pred0, conf_thres, iou_thres, None, False, max_det=max_det)\n",
    "        det = pred[0].cpu().numpy()\n",
    "        xyxy = det[:,:4]\n",
    "        confidence = det[:,4]\n",
    "        class_id = det[:,5].astype(int)\n",
    "\n",
    "        detections = Detections(\n",
    "            xyxy=xyxy,\n",
    "            confidence=confidence,\n",
    "            class_id=class_id\n",
    "        )\n",
    "\n",
    "        # results = model(frame)\n",
    "        # detections = Detections(\n",
    "        #     xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "        #     confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "        #     class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        # )\n",
    "        # filtering out detections with unwanted classes\n",
    "        # mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)\n",
    "        # detections.filter(mask=mask, inplace=True)\n",
    "        # tracking detections\n",
    "        tracks = byte_tracker.update(\n",
    "            output_results=detections2boxes(detections=detections),\n",
    "            img_info=frame0.shape,\n",
    "            img_size=frame0.shape\n",
    "        )\n",
    "        tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
    "        detections.tracker_id = np.array(tracker_id)\n",
    "        # filtering out detections without trackers\n",
    "        mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
    "        detections.filter(mask=mask, inplace=True)\n",
    "        # format custom labels\n",
    "        labels = [\n",
    "            f\"#{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "            for _, confidence, class_id, tracker_id\n",
    "            in detections\n",
    "        ]\n",
    "        # updating line counter\n",
    "        line_counter.update(detections=detections)\n",
    "        # annotate and display frame\n",
    "        frame0 = box_annotator.annotate(frame=frame0, detections=detections, labels=labels)\n",
    "        line_annotator.annotate(frame=frame0, line_counter=line_counter)\n",
    "        sink.write_frame(frame0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
